{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fdabc50-9bdf-445b-a121-0dcc3d082fe0",
   "metadata": {},
   "source": [
    "# Signal Processing Techniques for Contrastive Learning in CV\n",
    "\n",
    "## What is Contrastive Learning?\n",
    "\n",
    "Contrastive learning is a self-supervised learning technique that aims to learn useful representations from unlabeled data. It has gained popularity in computer vision tasks, such as image classification and object detection, by leveraging the inherent structure and relationships within the data.\n",
    "\n",
    "In this notebook, we will use the MNIST dataset as an example to demonstrate the training and evaluation of the contrastive networks.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Before we begin, let's import the necessary libraries and prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c1e5d51-4ac5-4c34-ab2f-6be995da3a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b9a65d-b58d-41ac-b7dd-2df74ff83240",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "For our contrastive network, we will use a simple convolutional neural network (CNN) architecture. The CNN will consist of several convolutional layers followed by fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "274fd498-bb18-4345-87b4-941528244f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the contrastive network model\n",
    "class ContrastiveNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContrastiveNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the contrastive network\n",
    "model = ContrastiveNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3af6ec-d62b-48b1-bcfb-375f613580d6",
   "metadata": {},
   "source": [
    "# Contrastive Loss\n",
    "The contrastive loss function aims to maximize the similarity between similar pairs of data samples and minimize the similarity between dissimilar pairs. It encourages the network to learn meaningful representations that capture the underlying structure of the data.\n",
    "Loss = $-\\frac{1}{N} \\sum_{i=1}^{N} [y \\cdot \\log (\\hat{y}) + (1 - y) \\cdot \\log (1 - \\hat{y})]$, where $N$ is the number of samples in the batch, $y$ is the true label (either 0 or 1), and $\\hat{y}$ is the predicted probability output by the model. This formula represents the binary cross-entropy loss commonly used in binary classification tasks. It penalizes the model for incorrect predictions by comparing the predicted probability ($\\hat{y}$) to the true label ($y$). The loss is averaged over all samples in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55366050-ab5e-4bd0-be81-3b688f1d29ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the contrastive loss function\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        distance_positive = nn.functional.pairwise_distance(anchor, positive)\n",
    "        distance_negative = nn.functional.pairwise_distance(anchor, negative)\n",
    "        loss = torch.mean(torch.relu(distance_positive - distance_negative + self.margin))\n",
    "        return loss\n",
    "\n",
    "# Create an instance of the contrastive loss function\n",
    "criterion = ContrastiveLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b894785a-8299-41ad-b634-6a833c8be193",
   "metadata": {},
   "source": [
    "# Training the Contrastive Network\n",
    "Now, let's train the contrastive network using the contrastive loss and evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b4a8b40-e198-4bc7-bce2-f647d0b00006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.0012\n",
      "Epoch [2/10], Loss: 1.0003\n",
      "Epoch [3/10], Loss: 0.9999\n",
      "Epoch [4/10], Loss: 0.9994\n",
      "Epoch [5/10], Loss: 1.0015\n",
      "Epoch [6/10], Loss: 1.0008\n",
      "Epoch [7/10], Loss: 1.0005\n",
      "Epoch [8/10], Loss: 1.0006\n",
      "Epoch [9/10], Loss: 1.0003\n",
      "Epoch [10/10], Loss: 0.9992\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set the optimizer and learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, _ in train_loader:\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Perform forward pass\n",
    "        embeddings = model(images)\n",
    "        \n",
    "        # Generate positive and negative samples\n",
    "        positive_samples = torch.flip(embeddings, dims=[0])\n",
    "        negative_samples = torch.cat((embeddings[1:], embeddings[0].unsqueeze(0)), dim=0)\n",
    "        \n",
    "        # Compute the contrastive loss\n",
    "        loss = criterion(embeddings, positive_samples, negative_samples)\n",
    "        \n",
    "        # Perform backward pass and update the parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Print the average loss for each epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fe0f80-3f8f-4892-ab13-c4d12e9526ae",
   "metadata": {},
   "source": [
    "## Evolution from Contrastive to Triplet Loss\n",
    "The evolution from contrastive loss to triplet loss was driven by the desire to address some limitations of the contrastive loss in learning good feature representations. While contrastive loss worked well for pair-wise comparisons, it had difficulties in situations where the number of negative samples was much larger than the number of positive samples, leading to slow convergence and inefficient training. To address these challenges, the triplet loss was introduced.\n",
    "\n",
    "The triplet loss is a type of metric learning loss that aims to learn feature embeddings such that similar samples are closer together, while dissimilar samples are pushed further apart in the embedding space. It is particularly useful in tasks like face recognition, where the goal is to distinguish between different individuals.\n",
    "\n",
    "The key idea behind triplet loss is to form triplets of samples, consisting of an anchor sample, a positive sample (from the same class as the anchor), and a negative sample (from a different class). The loss is then computed based on the distances between these samples in the embedding space.\n",
    "\n",
    "Loss = $\\max (\\text{dist}(\\text{anchor}, \\text{positive}) - \\text{dist}(\\text{anchor}, \\text{negative}) + \\text{margin}, 0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "824aeff3-ec52-4f30-996b-f370ee9ff191",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin = 1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        distance_positive = torch.norm(anchor - positive, p=2, dim=1)\n",
    "        distance_negative = torch.norm(anchor - negative, p=2, dim=1)\n",
    "        losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e5548e-6a94-43cb-bd9e-6f87eb749541",
   "metadata": {},
   "source": [
    "The following implements a sample custom dataset for Triplet networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9fda3fa-8f78-40e2-8e53-d914bddf89b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class MNISTTripletDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        anchor, anchor_label = self.dataset[index]\n",
    "        \n",
    "        # Find positive sample with the same label as anchor\n",
    "        positive_index = torch.randint(high=len(self.dataset), size=(1,)).item()\n",
    "        while self.dataset[positive_index][1] != anchor_label:\n",
    "            positive_index = torch.randint(high=len(self.dataset), size=(1,)).item()\n",
    "        positive, _ = self.dataset[positive_index]\n",
    "\n",
    "        # Find negative sample with different label from anchor\n",
    "        negative_index = torch.randint(high=len(self.dataset), size=(1,)).item()\n",
    "        while self.dataset[negative_index][1] == anchor_label:\n",
    "            negative_index = torch.randint(high=len(self.dataset), size=(1,)).item()\n",
    "        negative, _ = self.dataset[negative_index]\n",
    "\n",
    "        return anchor, positive, negative\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109f3740-0b83-422b-a958-4a1b4b2ecbb4",
   "metadata": {},
   "source": [
    "Define the triplet network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caab1f3c-bbbf-40af-baa6-971536b7f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TripletNetwork, self).__init__()\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(784, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6177d685-5a38-4dc9-9165-4cfcdb477dcb",
   "metadata": {},
   "source": [
    "Start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c1665b-4e52-4f35-8f96-952a7bdece4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.1295\n",
      "Epoch [2/10], Loss: 0.0504\n",
      "Epoch [3/10], Loss: 0.0364\n",
      "Epoch [4/10], Loss: 0.0273\n",
      "Epoch [5/10], Loss: 0.0276\n",
      "Epoch [6/10], Loss: 0.0233\n",
      "Epoch [7/10], Loss: 0.0211\n",
      "Epoch [8/10], Loss: 0.0194\n",
      "Epoch [9/10], Loss: 0.0174\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "\n",
    "# Create the triplet dataset\n",
    "train_triplet_dataset = MNISTTripletDataset(train_dataset)\n",
    "test_triplet_dataset = MNISTTripletDataset(test_dataset)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_triplet_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_triplet_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the network and the loss function\n",
    "model = TripletNetwork()\n",
    "criterion = TripletLoss()\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "# Set optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for anchor, positive, negative in train_loader:\n",
    "        anchor = anchor.view(anchor.size(0), -1).to(device)\n",
    "        positive = positive.view(positive.size(0), -1).to(device)\n",
    "        negative = negative.view(negative.size(0), -1).to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        anchor_embedding = model(anchor)\n",
    "        positive_embedding = model(positive)\n",
    "        negative_embedding = model(negative)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(anchor_embedding, positive_embedding, negative_embedding)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for anchor, positive, negative in test_loader:\n",
    "        anchor = anchor.view(anchor.size(0), -1).to(device)\n",
    "        positive = positive.view(positive.size(0), -1).to(device)\n",
    "        negative = negative.view(negative.size(0), -1).to(device)\n",
    "\n",
    "        anchor_embedding = model(anchor)\n",
    "        positive_embedding = model(positive)\n",
    "        negative_embedding = model(negative)\n",
    "\n",
    "        loss = criterion(anchor_embedding, positive_embedding, negative_embedding)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba9cc11-24b7-4f7d-a9d2-81e0542824b7",
   "metadata": {},
   "source": [
    "## Applications of Contrastive Learning\n",
    "Contrastive has been successful in various computer vision tasks, including the following:\n",
    "\n",
    "1. Limited labeled data: Contrastive learning can be effective when there is a scarcity of labeled data. By leveraging large amounts of unlabeled data, contrastive learning can learn useful representations that generalize well to downstream tasks.\n",
    "\n",
    "2. Similarity-based tasks: Contrastive learning is particularly suitable for tasks that involve measuring similarity or dissimilarity between examples. It excels in tasks like image retrieval, clustering, and nearest neighbor search, where the goal is to find similar instances.\n",
    "\n",
    "3. Unsupervised feature learning: When there is no explicit supervision available, contrastive learning can be used to learn meaningful representations from raw data. It enables the model to capture underlying structures, patterns, and semantics in the data without requiring explicit labels.\n",
    "\n",
    "4. Data augmentation: Contrastive learning can be combined with data augmentation techniques to increase the diversity of positive and negative examples. By augmenting the data with various transformations, the model is exposed to a wider range of variations, making the learned representations more robust.\n",
    "\n",
    "5. Transfer learning: Contrastive learning can serve as a pretraining step for transfer learning. By pretraining on a large unlabeled dataset using contrastive learning, the model can learn general features that can be fine-tuned on specific downstream tasks with limited labeled data.\n",
    "\n",
    "In this notebook, I want to dive into some signal processing techniques for similarity-based tasks specifically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1efdc8b-5577-41b5-a18d-ffddc2c92e49",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Speaking of feature extraction, several smoothing techniques comes to mind. Fourier transform, rolling window - these are all ways to filter out the noises and keep the peaks in signals that we want.\n",
    "1. Fourier Transform: Convert the input signals from the time domain to the frequency domain using Fourier transform. This helps in analyzing the frequency components present in the signal.\n",
    "2. Wavelet Transform: Decompose the signal into different frequency components using wavelet transform. This provides a multi-resolution representation of the signal.\n",
    "3. Spectrogram: Compute the spectrogram of the signal by applying the short-time Fourier transform (STFT). This represents the signal's frequency content over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95641c89-3939-471a-a3bc-23947f6a621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.fft import fft\n",
    "from scipy.signal import spectrogram\n",
    "import pywt\n",
    "\n",
    "# Example signal\n",
    "signal = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Fourier Transform\n",
    "fourier_transform = fft(signal)\n",
    "print(\"Fourier Transform:\", fourier_transform)\n",
    "\n",
    "# Wavelet Transform (using Haar wavelet)\n",
    "wavelet_transform = pywt.dwt(signal, \"haar\")\n",
    "print(\"Wavelet Transform:\", wavelet_transform)\n",
    "\n",
    "# Spectrogram\n",
    "frequencies, times, spectrogram_values = spectrogram(signal)\n",
    "print(\"Spectrogram:\")\n",
    "print(\"Frequencies:\", frequencies)\n",
    "print(\"Times:\", times)\n",
    "print(\"Spectrogram Values:\")\n",
    "print(spectrogram_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcd3d70-f625-4263-86f8-2e4e2d1beab5",
   "metadata": {},
   "source": [
    "## Maximizing Information Entropy of the Peak Signals Using Contrastive Learning\n",
    "\n",
    "Contrastive learning aims to maximize the information entropy of the peak signals by leveraging the similarity between positive samples and maximizing the dissimilarity between negative samples.\n",
    "\n",
    "In contrastive learning, peak signals are identified based on their distinct features or high response values. These peaks represent important or informative patterns in the data. Contrastive learning encourages the positive samples (peaks) to be similar to each other. By maximizing the similarity between positive samples, the network learns to capture the common features or patterns present in the peaks. For instance, in AFM Force-Z curve classification, a single-rupture event is described by one such peak, while other data lack such characteristic.\n",
    "\n",
    "In fact, in AFM image recognition tasks, after baseline correction and flattening, to ensure the contrastive networks understood the characteristic of the \"peak,\" we choose to feed in a second channel other than the original function - the derivative of the si."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9ec94e-7946-44fe-8fcf-65f1764d815f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
